{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, mean, abs, tanh, zeros, outer, log\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self, learning_rate, h_size, book, epochs, batch_length, back_steps):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.h_size = h_size\n",
    "        self.epochs = epochs\n",
    "        self.batch_length = batch_length\n",
    "        self.back_steps = back_steps\n",
    "        self.words = gutenberg.words(book)\n",
    "        self.letters = gutenberg.raw(book)\n",
    "        self.unique_chars = set(self.letters)\n",
    "        self.unique_len = len(self.unique_chars)\n",
    "        self.last_h = zeros((1, self.h_size))\n",
    "        # Make a mapping from character to an integer so that every character is represented by that integer\n",
    "        self.char_to_int = {char : counter for counter, char in enumerate(self.unique_chars)}\n",
    "        # Make a mapping from integer back to the character\n",
    "        self.int_to_char = {counter : char for counter, char in enumerate(self.unique_chars)}\n",
    "        \n",
    "        \"\"\"\n",
    "        Model parameters\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## randomly initialise weight matrices\n",
    "        self.Wxh = random.rand(self.h_size,self.unique_len) * 0.01\n",
    "        self.Whh = random.rand(self.h_size,h_size) * 0.01\n",
    "        self.Why = random.rand(self.unique_len, self.h_size) * 0.01\n",
    "        self.bh = random.rand(self.h_size,) * 0.01\n",
    "        self.by = random.rand(self.unique_len,) * 0.01\n",
    "        \n",
    "        \n",
    "#         ## Value of the hiddden state at every time step\n",
    "#         self.hist_h = np.zeros((l, h_size))\n",
    "#         ## Value of the output at every time step\n",
    "#         self.hist_y = np.zeros((l,len(unique_chars)))\n",
    "#         ## probability distribution at every time step\n",
    "        \n",
    "    def fit(self):\n",
    "        \n",
    "        for x in range(self.epochs):\n",
    "            print (x)\n",
    "            for i in range(0, len(self.letters), self.batch_length):\n",
    "                if(i >= (len(self.letters)//self.batch_length) - 1):\n",
    "                    break\n",
    "                # Divide the corpus into sequences of fixed length\n",
    "                inp = [self.char_to_int[k] for k in self.letters[i:i + self.batch_length]]\n",
    "                target = [self.char_to_int[k] for k in self.letters[i + 1:i + self.batch_length + 1]]\n",
    "                # Forward pass\n",
    "                hist_h, output, hist_p = self.train(inp, target)\n",
    "    #             loss = self.calculate_loss(prob, target)\n",
    "\n",
    "                # Backward pass\n",
    "                DWhy, Dby, DWhh, DWxh, Dbh = self.bptt(inp, target, hist_p, hist_h)\n",
    "\n",
    "                ## Update the paramteres by simple sgd\n",
    "                self.by -= self.learning_rate * Dby        \n",
    "                self.Why -= self.learning_rate * DWhy\n",
    "                self.Whh -= self.learning_rate * DWhh        \n",
    "                self.Wxh -= self.learning_rate * DWxh        \n",
    "                self.bh -= self.learning_rate * Dbh\n",
    "        \n",
    "            \n",
    "    def train(self, inp, target):\n",
    "#         print(inp)\n",
    "        ## Value of the hiddden state at every time step\n",
    "        hist_h = np.zeros((self.batch_length, self.h_size))\n",
    "        ## Value of the output at every time step\n",
    "        hist_y = np.zeros((self.batch_length, self.unique_len))\n",
    "        ## probability distribution at every time step\n",
    "        hist_p = np.zeros((self.batch_length, self.unique_len))\n",
    "        ## We will use the last state from previous training batch as the starting state for the next one\n",
    "        hist_h[-1] = self.last_h\n",
    "#         hist_h[0] = tanh(self.Wxh[:, inp[0]] + self.bh)\n",
    "#         hist_y[0] = dot(self.Why, hist_h[0]) + self.by\n",
    "#         hist_p[0] = softmax(hist_y[0])\n",
    "        for t in range(self.batch_length):\n",
    "            try:\n",
    "                hist_h[t] = tanh(dot(self.Whh, hist_h[t-1]) + self.Wxh[:, inp[t]] + self.bh)\n",
    "            except: \n",
    "                print(inp[t], t)\n",
    "            hist_y[t] = dot(self.Why, hist_h[t]) + self.by\n",
    "            hist_p[t] = self.softmax(hist_y[t])\n",
    "        self.last_h = hist_h[-1]\n",
    "        return hist_h, hist_y, hist_p\n",
    "    \n",
    "    def calculate_loss(self, output, target):\n",
    "        loss = 0\n",
    "        for x in range(self.batch_length):\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def generate_sentence(self, starting_letter, sample_length):\n",
    "        '''\n",
    "        Takes a starting letter and creates a sequence using it\n",
    "        \n",
    "        starting letter : letter which the sampled sentence will begin with\n",
    "        sample_length : What length of sentence is required\n",
    "        \n",
    "        '''\n",
    "        hist_h = self.last_h\n",
    "        generate_sentence = \"\"\n",
    "        sent = np.zeros((sample_length + 1, ))\n",
    "        sent[0] = self.char_to_int[starting_letter]\n",
    "#         print(self.char_to_int[starting_letter])\n",
    "#         print(sent[0])\n",
    "        for t in range(sample_length):\n",
    "            print(sent[t])\n",
    "            # Forward pass\n",
    "            hist_h = tanh(dot(self.Whh, hist_h) + self.Wxh[:, int(sent[t])] + self.bh)\n",
    "            y = dot(self.Why, hist_h) + self.by\n",
    "            p = self.softmax(y)\n",
    "            \n",
    "            #find the index having the highest probability\n",
    "            index = np.random.choice(range(self.unique_len), p=p.ravel())\n",
    "            sent[t + 1] = index \n",
    "            # Convert index to its corresponding letter\n",
    "            letter = self.int_to_char[index]\n",
    "            generate_sentence += letter\n",
    "            \n",
    "        return generate_sentence\n",
    "            \n",
    "    def bptt(self, inp, target, hist_p, hist_h):\n",
    "        ## inp is the input to our RNN\n",
    "        ## target is the list of numbers that corresponds to the actual letters at each time step\n",
    "        ## Initialise the gradient matrices for each parameter\n",
    "        DWhy = np.zeros_like(self.Why)\n",
    "        Dby = np.zeros_like(self.by)\n",
    "        DWhh = np.zeros_like(self.Whh)\n",
    "        DWxh = np.zeros_like(self.Wxh)\n",
    "        Dbh = np.zeros_like(self.bh)\n",
    "        for t in range(self.batch_length):\n",
    "            ## Output - target\n",
    "            Dp = hist_p[t]\n",
    "            Dp[target[t]] -= 1\n",
    "            ## (Output - target) * (hist_h[t]) \n",
    "            DWhy += outer(Dp , hist_h[t].T)\n",
    "            Dby += Dp\n",
    "            #(output - target)* Why *(1 - hist_h[t]^2)\n",
    "            delta = dot(self.Why.T, Dp) * (1 - hist_h[t]**2)\n",
    "            for step in np.arange(max(0, t - self.back_steps), t + 1)[::-1]:\n",
    "                DWhh += np.outer(delta, hist_h[step - 1])\n",
    "                DWxh[:, inp[step]] += delta\n",
    "                Dbh += delta\n",
    "                delta += dot(self.Whh, delta) * (1 - hist_h[step]**2)\n",
    "        for Dparam in [DWxh, DWhh, DWhy, Dbh, Dby]:\n",
    "            np.clip(Dparam, -5, 5, out=Dparam) # clip to mitigate exploding gradients \n",
    "        return [DWhy, Dby, DWhh, DWxh, Dbh]\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus on which we want to train our RNN\n",
    "book = 'austen-emma.txt'\n",
    "#Get all the words in the corpora\n",
    "words = gutenberg.words(book)\n",
    "# Get all the letters in the corpora\n",
    "letters = gutenberg.raw(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(0.001, 128, book, 10, 20, 5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "rnn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "71.0\n",
      "63.0\n",
      "38.0\n",
      "43.0\n",
      "62.0\n",
      "38.0\n",
      "21.0\n",
      "33.0\n",
      "34.0\n",
      "36.0\n",
      "55.0\n",
      "10.0\n",
      "72.0\n",
      "55.0\n",
      "72.0\n",
      "16.0\n",
      "76.0\n",
      "33.0\n",
      "16.0\n",
      "38.0\n",
      "72.0\n",
      "33.0\n",
      "71.0\n",
      "38.0\n",
      "62.0\n",
      "65.0\n",
      "68.0\n",
      "7.0\n",
      "65.0\n",
      "38.0\n",
      "9.0\n",
      "10.0\n",
      "68.0\n",
      "24.0\n",
      "40.0\n",
      "38.0\n",
      "11.0\n",
      "76.0\n",
      "38.0\n",
      "10.0\n",
      "75.0\n",
      "55.0\n",
      "36.0\n",
      "38.0\n",
      "9.0\n",
      "19.0\n",
      "38.0\n",
      "56.0\n",
      "10.0\n",
      "72.0\n",
      "40.0\n",
      "38.0\n",
      "72.0\n",
      "10.0\n",
      "72.0\n",
      "38.0\n",
      "76.0\n",
      "75.0\n",
      "16.0\n",
      "38.0\n",
      "34.0\n",
      "10.0\n",
      "62.0\n",
      "71.0\n",
      "10.0\n",
      "75.0\n",
      "65.0\n",
      "55.0\n",
      "36.0\n",
      "38.0\n",
      "7.0\n",
      "65.0\n",
      "55.0\n",
      "68.0\n",
      "72.0\n",
      "76.0\n",
      "65.0\n",
      "55.0\n",
      "38.0\n",
      "72.0\n",
      "10.0\n",
      "36.0\n",
      "21.0\n",
      "21.0\n",
      "55.0\n",
      "16.0\n",
      "38.0\n",
      "33.0\n",
      "72.0\n",
      "38.0\n",
      "33.0\n",
      "43.0\n",
      "24.0\n",
      "19.0\n",
      "76.0\n",
      "63.0\n",
      "55.0\n",
      "38.0\n",
      "8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cv Em hiIteasesdoid sic mr\\ngr wa\\nly bo anet w. qasy sas ond Iamcanret gre\\nsore sathhed is iEl.ove As'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.generate_sentence('a', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
