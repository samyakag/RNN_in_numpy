{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, mean, abs, tanh, zeros, outer, log\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self, learning_rate, h_size, book, epochs, batch_length, back_steps):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.h_size = h_size\n",
    "        self.epochs = epochs\n",
    "        self.batch_length = batch_length\n",
    "        self.back_steps = back_steps\n",
    "        self.words = gutenberg.words(book)\n",
    "        self.letters = gutenberg.raw(book)\n",
    "        self.unique_chars = set(self.letters)\n",
    "        self.unique_len = len(self.unique_chars)\n",
    "        self.last_h = zeros((1, self.h_size))\n",
    "        # Make a mapping from charachter to an integer so that every charachter is represented by that integer\n",
    "        self.char_to_int = {char:counter for counter, char in enumerate(self.unique_chars)}\n",
    "        # Make a mapping from integer back to the charachter\n",
    "        self.int_to_char = {counter:char for counter, char in enumerate(self.unique_chars)}\n",
    "        \n",
    "        \"\"\"\n",
    "        Model parameters\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## randomly initialise weight matrices\n",
    "        self.Wxh = random.rand(h_size,self.unique_len) * 0.01\n",
    "        self.Whh = random.rand(h_size,h_size) * 0.01\n",
    "        self.Why = random.rand(self.unique_len, h_size) * 0.01\n",
    "        self.bh = random.rand(h_size,) * 0.01\n",
    "        self.by = random.rand(self.unique_len,) * 0.01\n",
    "        \n",
    "        \n",
    "#         ## Value of the hiddden state at every time step\n",
    "#         self.hist_h = np.zeros((l, h_size))\n",
    "#         ## Value of the output at every time step\n",
    "#         self.hist_y = np.zeros((l,len(unique_chars)))\n",
    "#         ## probability distribution at every time step\n",
    "        \n",
    "    def fit(self):\n",
    "        \n",
    "        for x in range(self.epochs):\n",
    "            print(x)\n",
    "            for i in range(0, len(self.letters), self.batch_length):\n",
    "                # Divide the corpus into sequences of fixed length\n",
    "                inp = [self.char_to_int[k] for k in self.letters[i:i + self.batch_length]]\n",
    "                target = [self.char_to_int[k] for k in self.letters[i + 1:i + self.batch_length + 1]]\n",
    "                # Forward pass\n",
    "                hist_h, output, hist_p = self.train(inp, target)\n",
    "    #             loss = self.calculate_loss(prob, target)\n",
    "\n",
    "                # Backward pass\n",
    "                DWhy, Dby, DWhh, DWxh, Dbh = self.bptt(inp, target, hist_p, hist_h)\n",
    "\n",
    "                ## Update the paramteres by simple sgd\n",
    "                self.by -= self.learning_rate * Dby        \n",
    "                self.Why -= self.learning_rate * DWhy\n",
    "                self.Whh -= self.learning_rate * DWhh        \n",
    "                self.Wxh -= self.learning_rate * DWxh        \n",
    "                self.bh -= self.learning_rate * Dbh\n",
    "        \n",
    "            \n",
    "    def train(self, inp, target):\n",
    "        ## Value of the hiddden state at every time step\n",
    "        hist_h = np.zeros((self.batch_length, self.h_size))\n",
    "        ## Value of the output at every time step\n",
    "        hist_y = np.zeros((self.batch_length, self.unique_len))\n",
    "        ## probability distribution at every time step\n",
    "        hist_p = np.zeros((self.batch_length, self.unique_len))\n",
    "        ## We will use the last state from previous training batch as the starting state for the next one\n",
    "        hist_h[-1] = self.last_h\n",
    "#         hist_h[0] = tanh(self.Wxh[:, inp[0]] + self.bh)\n",
    "#         hist_y[0] = dot(self.Why, hist_h[0]) + self.by\n",
    "#         hist_p[0] = softmax(hist_y[0])\n",
    "        for t in range(self.batch_length):\n",
    "            hist_h[t] = tanh(dot(self.Whh, hist_h[t-1]) + self.Wxh[:, inp[t]] + self.bh)\n",
    "            hist_y[t] = dot(self.Why, hist_h[t]) + self.by\n",
    "            hist_p[t] = self.softmax(hist_y[t])\n",
    "        self.last_h = hist_h[-1]\n",
    "        return hist_h, hist_y, hist_p\n",
    "    \n",
    "    def calculate_loss(self, output, target):\n",
    "        loss = 0\n",
    "        for x in range(self.batch_length):\n",
    "            pass\n",
    "    \n",
    "    def bptt(self, inp, target, hist_p, hist_h):\n",
    "        ## inp is the input to our RNN\n",
    "        ## target is the list of numbers that corresponds to the actual letters at each time step\n",
    "        ## Initialise the gradient matrices for each parameter\n",
    "        DWhy = np.zeros_like(self.Why)\n",
    "        Dby = np.zeros_like(self.by)\n",
    "        DWhh = np.zeros_like(self.Whh)\n",
    "        DWxh = np.zeros_like(self.Wxh)\n",
    "        Dbh = np.zeros_like(self.bh)\n",
    "        ## This is Output - target\n",
    "        for t in range(self.batch_length):\n",
    "            ## Output - target\n",
    "            Dp = hist_p[t]\n",
    "            Dp[target[t]] -= 1\n",
    "            ## (Output - target) * (hist_h[t]) \n",
    "            DWhy += outer(Dp , hist_h[t].T)\n",
    "            Dby += Dp\n",
    "            #(output - target)* Why *(1 - hist_h[t]^2)\n",
    "            delta = dot(self.Why.T, Dp) * (1 - hist_h[t]**2)\n",
    "            for step in np.arange(max(0, t - self.back_steps), t + 1)[::-1]:\n",
    "                DWhh += np.outer(delta, hist_h[step - 1])\n",
    "                DWxh[:, inp[step]] += delta\n",
    "                Dbh += delta\n",
    "                delta += dot(self.Whh, delta) * (1 - hist_h[step]**2)\n",
    "        for Dparam in [DWxh, DWhh, DWhy, Dbh, Dby]:\n",
    "            np.clip(Dparam, -5, 5, out=Dparam) # clip to mitigate exploding gradients \n",
    "        return [DWhy, Dby, DWhh, DWxh, Dbh]\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus on which we want to train our RNN\n",
    "book = 'austen-emma.txt'\n",
    "#Get all the words in the corpora\n",
    "words = gutenberg.words(book)\n",
    "# Get all the letters in the corpora\n",
    "letters = gutenberg.raw(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3c81c9526590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-a4502d259d99>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mDWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDbh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m## Update the paramteres by simple sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-a4502d259d99>\u001b[0m in \u001b[0;36mbptt\u001b[0;34m(self, inp, target, hist_p, hist_h)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mDWxh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mDbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mdelta\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhist_h\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mDparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clip to mitigate exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn = RNN(0.001, 128, book, 10, 20, 5) \n",
    "rnn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = {char:counter for counter, char in enumerate(uni)}\n",
    "int_to_char = {counter:char for counter, char in enumerate(uni)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[char_to_int[k] for k in letters[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
